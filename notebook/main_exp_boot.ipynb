{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray\n",
    "from scipy.optimize import minimize\n",
    "from typing import List, Tuple\n",
    "import time as time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 価格を生成する関数\n",
    "def create_price(r_mean: float, r_std: float, M: int) -> NDArray[np.float_]:\n",
    "    # r_mean = (r_min + r_max) / 2\n",
    "    # r_std = (r_max - r_mean) / 2\n",
    "    # r_minとr_maxの間のランダムな0.1刻みの少数をM個生成\n",
    "    price = np.random.normal(r_mean, r_std, size=M)\n",
    "    # price = np.round(price, 1)\n",
    "\n",
    "    return price\n",
    "\n",
    "\n",
    "# alphaを作成する関数\n",
    "def alpha_star(M: int) -> NDArray[np.float_]:\n",
    "    alpha_star = np.random.uniform(M, 3 * M, size=M)\n",
    "    return alpha_star\n",
    "\n",
    "\n",
    "# betaを作成する関数\n",
    "def beta_star(M: int, M_prime: int) -> NDArray[np.float_]:\n",
    "    beta_star = np.zeros((M, M_prime))\n",
    "\n",
    "    for m in range(M):\n",
    "        for m_prime in range(M_prime):\n",
    "            if m == m_prime:\n",
    "                beta_star[m, m_prime] = np.random.uniform(-3 * M, -2 * M)\n",
    "            else:\n",
    "                beta_star[m, m_prime] = np.random.uniform(0, 3)\n",
    "\n",
    "    return beta_star\n",
    "\n",
    "\n",
    "def quantity_function(\n",
    "    price: NDArray[np.float_],\n",
    "    alpha: NDArray[np.float_],\n",
    "    beta: NDArray[np.float_],\n",
    "    delta: float = 0.1,  # ノイズレベルを指定（例として0.1を使用）\n",
    ") -> list[float]:\n",
    "    M = len(price)\n",
    "    quantity_list = []\n",
    "    q_m_no_noise = []\n",
    "\n",
    "    # ステップ1: ノイズなしのq_mを計算\n",
    "    for m in range(M):\n",
    "        sum_beta = 0\n",
    "        for m_prime in range(M):\n",
    "            sum_beta += beta[m][m_prime] * price[m_prime]\n",
    "        quantity = alpha[m] + sum_beta\n",
    "        q_m_no_noise.append(quantity)\n",
    "\n",
    "    # E[q_m^2]を計算\n",
    "    E_q_m_squared = np.mean(np.array(q_m_no_noise) ** 2)\n",
    "\n",
    "    # ステップ2: ノイズの標準偏差sigmaを計算\n",
    "    sigma = delta * np.sqrt(E_q_m_squared)\n",
    "\n",
    "    # ステップ3: ノイズを加えて最終的なq_mを計算\n",
    "    for m in range(M):\n",
    "        epsilon = np.random.normal(0, sigma)\n",
    "        quantity = q_m_no_noise[m] + epsilon\n",
    "        quantity_list.append(quantity)\n",
    "\n",
    "    return quantity_list\n",
    "\n",
    "\n",
    "def sales_function(\n",
    "    price: NDArray[np.float_], alpha: NDArray[np.float_], beta: NDArray[np.float_]\n",
    ") -> list[float]:\n",
    "    M = len(price)\n",
    "    sales_list = []\n",
    "\n",
    "    for m in range(M):\n",
    "        sum_beta = 0\n",
    "        for m_prime in range(M):\n",
    "            sum_beta += beta[m][m_prime] * price[m_prime]\n",
    "\n",
    "        quantity = alpha[m] + sum_beta\n",
    "        sales_list.append(quantity * price[m])\n",
    "\n",
    "    return sales_list\n",
    "\n",
    "\n",
    "def create_date(M, N, r_mean, r_std, delta=0.1):\n",
    "    alpha = alpha_star(M)\n",
    "    beta = beta_star(M, M)\n",
    "\n",
    "    price_list = []\n",
    "    quantity_list = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        price = create_price(r_mean, r_std, M)\n",
    "        quantity = quantity_function(price, alpha, beta, delta)\n",
    "        price_list.append(price)\n",
    "        quantity_list.append(quantity)\n",
    "\n",
    "    X = np.array(price_list)\n",
    "    Y = np.array(quantity_list)\n",
    "\n",
    "    return alpha, beta, X, Y\n",
    "\n",
    "\n",
    "def create_bounds(M, r_min, r_max):\n",
    "    lb = np.full(M, r_min)\n",
    "    ub = np.full(M, r_max)\n",
    "\n",
    "    range_bounds = []\n",
    "    for i in range(M):\n",
    "        range_bounds.append(lb[i])\n",
    "\n",
    "    for i in range(M):\n",
    "        range_bounds.append(ub[i])\n",
    "\n",
    "    bounds = [(r_min, r_max) for _ in range(M)]\n",
    "\n",
    "    return lb, ub, bounds, range_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数を定義（最大化問題を最小化問題に変換）\n",
    "def sales_objective_function(prices, alpha, beta, M):\n",
    "    return -sum(\n",
    "        prices[m] * (alpha[m] + sum(beta[m][m_prime] * prices[m_prime] for m_prime in range(M)))\n",
    "        for m in range(M)\n",
    "    )\n",
    "\n",
    "\n",
    "def sales_optimize(\n",
    "    M: int,\n",
    "    alpha: np.ndarray,\n",
    "    beta: np.ndarray,\n",
    "    bounds: list[tuple[float, float]],\n",
    ") -> Tuple[float, np.ndarray]:\n",
    "    # 初期値として与えられたprices_listを使用\n",
    "    initial_prices = np.full(M, 0.6)\n",
    "\n",
    "    # 最適化を実行\n",
    "    result = minimize(\n",
    "        sales_objective_function,\n",
    "        initial_prices,\n",
    "        args=(alpha, beta, M),\n",
    "        bounds=bounds,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    # 最適な価格と目的関数の値を取得\n",
    "    optimal_prices = result.x\n",
    "    optimal_value = -result.fun  # 符号を反転して元の最大化問題の値に戻す\n",
    "    return optimal_value, optimal_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数を定義\n",
    "def predict_objective_function(\n",
    "    prices: NDArray[np.float_], intercepts: [float], coefs: [NDArray[np.float_]], M: int\n",
    ") -> float:\n",
    "    # 各変数の内容をデバッグ出力\n",
    "    # print(\"prices:\", prices)\n",
    "    # print(\"intercepts:\", intercepts)\n",
    "    # print(\"coefs:\", coefs)\n",
    "    # print(\"M:\", M)\n",
    "\n",
    "    return -sum(\n",
    "        prices[m]\n",
    "        * (intercepts[m] + sum(coefs[m][m_prime] * prices[m_prime] for m_prime in range(M)))\n",
    "        for m in range(M)\n",
    "    )\n",
    "\n",
    "\n",
    "# 予測と最適化を行う関数\n",
    "def predict_optimize(\n",
    "    M: int, X: NDArray[np.float_], Y: NDArray[np.float_], bounds: list[float]\n",
    ") -> tuple[float, NDArray[np.float_]]:\n",
    "    lr = MultiOutputRegressor(LinearRegression())\n",
    "    lr.fit(X, Y)\n",
    "    # 係数と切片を取得\n",
    "    coefs = [estimate.coef_ for estimate in lr.estimators_]\n",
    "    intercepts = [estimate.intercept_ for estimate in lr.estimators_]\n",
    "\n",
    "    # 初期値として与えられたprices_listを使用\n",
    "    initial_prices = np.full(M, 0.6)\n",
    "    # 最適化を実行\n",
    "    result = minimize(\n",
    "        predict_objective_function,\n",
    "        initial_prices,\n",
    "        args=(intercepts, coefs, M),\n",
    "        bounds=bounds,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    # 最適な価格と目的関数の値を取得\n",
    "    optimal_prices = result.x\n",
    "    optimal_value = -result.fun  # 符号を反転して元の最大化問題の値に戻す\n",
    "    return optimal_value, optimal_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVを行う関数\n",
    "def cross_validation(\n",
    "    tilda_coefs_list: list[NDArray[np.float_]],\n",
    "    tilda_intercepts_list: list[float],\n",
    "    hat_coefs_list: list[NDArray[np.float_]],\n",
    "    hat_intercepts_list: list[float],\n",
    "    M: int,\n",
    "    K: int,\n",
    "    bounds: list[float],\n",
    ") -> float:\n",
    "    optimal_sales_list = []\n",
    "    optimal_prices_list = [[] for _ in range(M)]\n",
    "    for i in range(K):\n",
    "        # 初期値として与えられたprices_listを使用\n",
    "        initial_prices = np.full(M, 0.6)\n",
    "\n",
    "        # 最適化を実行\n",
    "        result = minimize(\n",
    "            predict_objective_function,\n",
    "            initial_prices,\n",
    "            args=(tilda_intercepts_list[i], tilda_coefs_list[i], M),\n",
    "            bounds=bounds,\n",
    "            method=\"L-BFGS-B\",\n",
    "        )\n",
    "        # 最適な価格と目的関数の値を取得\n",
    "        optimal_prices = result.x\n",
    "        # print(\"optimal_prices cv:\", optimal_prices)\n",
    "        for m in range(M):\n",
    "            optimal_prices_list[m].append(optimal_prices[m])\n",
    "\n",
    "        sales_hat = np.sum(\n",
    "            sales_function(optimal_prices, hat_intercepts_list[i], hat_coefs_list[i])\n",
    "        )\n",
    "\n",
    "        optimal_sales_list.append(sales_hat)\n",
    "\n",
    "    return np.mean(optimal_sales_list), optimal_prices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_bounds_penalty_all(\n",
    "    bounds: List[float],\n",
    "    tilda_coefs_list: List[NDArray[np.float_]],\n",
    "    tilda_intercepts_list: List[NDArray[np.float_]],\n",
    "    hat_coefs_list: List[NDArray[np.float_]],\n",
    "    hat_intercepts_list: List[NDArray[np.float_]],\n",
    "    M: int,\n",
    "    K: int,\n",
    "    bounds_range: float,\n",
    "    lamda_1: float,\n",
    "    lamda_2:float,\n",
    ") -> float:\n",
    "    # bounds の整合性チェック\n",
    "    bounds_list = []\n",
    "    for i in range(M):\n",
    "        if bounds[i] > bounds[i + M]:\n",
    "            bounds_mean = (bounds[i] + bounds[i + M]) / 2\n",
    "            bounds_list.append((bounds_mean, bounds_mean))\n",
    "        else:\n",
    "            bounds_list.append((bounds[i], bounds[i + M]))\n",
    "    optimal_sales_list = []\n",
    "\n",
    "    # すでに外部でKFold分割や学習が終わっているものとして\n",
    "    # tilda_coefs_list[i], tilda_intercepts_list[i], hat_coefs_list[i], hat_intercepts_list[i]\n",
    "    # を使用して最適化と売上計算\n",
    "    for i in range(K):\n",
    "        intercepts = tilda_intercepts_list[i]\n",
    "        coefs = tilda_coefs_list[i]\n",
    "\n",
    "        # 最適化\n",
    "        initial_prices = np.full(M, 0.6)\n",
    "        result = minimize(\n",
    "            predict_objective_function,\n",
    "            initial_prices,\n",
    "            args=(intercepts, coefs, M),\n",
    "            bounds=bounds_list,\n",
    "            method=\"L-BFGS-B\",\n",
    "        )\n",
    "        optimal_prices = result.x\n",
    "\n",
    "        # hatモデルパラメータで売上計算\n",
    "        alpha = hat_intercepts_list[i]\n",
    "        beta = hat_coefs_list[i]\n",
    "\n",
    "        sales_hat = np.sum(sales_function(optimal_prices, alpha, beta))\n",
    "        optimal_sales_list.append(sales_hat)\n",
    "\n",
    "    # ペナルティ計算\n",
    "    penalty_1 = 0.0\n",
    "    for i in range(M):\n",
    "        penalty_1 += bounds[i + M] - bounds[i]\n",
    "\n",
    "    penalty_2 = 0.0\n",
    "    for i in range(M):\n",
    "        penalty_2 += max(0,bounds[i]-bounds[i+M])**2\n",
    "\n",
    "    mean_sales = np.mean(optimal_sales_list)\n",
    "\n",
    "    return -mean_sales + lamda_1 * max(0, penalty_1 - M * bounds_range) ** 2 + lamda_2 * penalty_2\n",
    "\n",
    "\n",
    "def estimate_bounds_penalty_nelder_all(\n",
    "    bounds: List[float],\n",
    "    tilda_coefs_list: List[NDArray[np.float_]],\n",
    "    tilda_intercepts_list: List[NDArray[np.float_]],\n",
    "    hat_coefs_list: List[NDArray[np.float_]],\n",
    "    hat_intercepts_list: List[NDArray[np.float_]],\n",
    "    M: int,\n",
    "    K: int,\n",
    "    r_min: float,\n",
    "    r_max: float,\n",
    "    bounds_range: float,\n",
    "    lamda_1: float,\n",
    "    lamda_2: float,\n",
    "    adaptive: bool = True,\n",
    ") -> Tuple[float, List[Tuple[float, float]]]:\n",
    "    # Nelder-Meadでの最適化\n",
    "    bounds_nelder = minimize(\n",
    "        cross_validation_bounds_penalty_all,\n",
    "        bounds,\n",
    "        args=(\n",
    "            tilda_coefs_list,\n",
    "            tilda_intercepts_list,\n",
    "            hat_coefs_list,\n",
    "            hat_intercepts_list,\n",
    "            M,\n",
    "            K,\n",
    "            bounds_range,\n",
    "            lamda_1,\n",
    "            lamda_2,\n",
    "        ),\n",
    "        method=\"Nelder-Mead\",\n",
    "        bounds=[(r_min, r_max) for _ in range(2 * M)],\n",
    "        options={\"adaptive\": adaptive},\n",
    "    )\n",
    "\n",
    "    opt_bounds = []\n",
    "    for i in range(M):\n",
    "        lb = min(bounds_nelder.x[i], bounds_nelder.x[i + M])\n",
    "        ub = max(bounds_nelder.x[i], bounds_nelder.x[i + M])\n",
    "        opt_bounds.append((lb, ub))\n",
    "\n",
    "    return -bounds_nelder.fun, opt_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_quan(price_list: np.ndarray, q: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    price_list : shape (N, M) の価格データ (N 件のサンプル、M 商品分)\n",
    "    q          : 上限とする分位数 (例: 0.95 など)\n",
    "\n",
    "    戻り値:\n",
    "        lower_bound : (1-q) 分位数 (shape (M,))\n",
    "        upper_bound : q 分位数   (shape (M,))\n",
    "    \"\"\"\n",
    "    # axis=0 で列ごとに分位数を計算\n",
    "    lower_bound = np.quantile(price_list, 1-q, axis=0)\n",
    "    upper_bound = np.quantile(price_list, q, axis=0)\n",
    "    bounds_quan = []\n",
    "    for i in range(len(lower_bound)):\n",
    "        bounds_quan.append((lower_bound[i], upper_bound[i]))\n",
    "    return bounds_quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap法での信頼区間の計算し、上限と下限に設定\n",
    "\n",
    "def bound_boot(price_list: np.ndarray, q: float, B: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    price_list : shape (N, M) の価格データ (N 件のサンプル、M 商品分)\n",
    "    q          : 上限とする分位数 (例: 0.95 など)\n",
    "    B          : ブートストラップのリサンプリング回数\n",
    "\n",
    "    戻り値:\n",
    "        lower_bound : (1-q) 分位数 (shape (M,))\n",
    "        upper_bound : q 分位数   (shape (M,))\n",
    "    \"\"\"\n",
    "    N, M = price_list.shape\n",
    "    lower_bound = np.zeros(M)\n",
    "    upper_bound = np.zeros(M)\n",
    "\n",
    "    for m in range(M):\n",
    "        boot_samples = np.random.choice(price_list[:, m], (B, N), replace=True)\n",
    "        boot_means = np.mean(boot_samples, axis=1)\n",
    "        lower_bound[m] = np.quantile(boot_means, 1-q)\n",
    "        upper_bound[m] = np.quantile(boot_means, q)\n",
    "\n",
    "    bounds_boot = []\n",
    "    for i in range(len(lower_bound)):\n",
    "        bounds_boot.append((lower_bound[i], upper_bound[i]))\n",
    "    return bounds_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 以下はお使いの既存の関数群を想定 -----\n",
    "# create_date, create_bounds, sales_optimize, predict_optimize,\n",
    "# sales_function, bound_quan, estimate_bounds_penalty_nelder_all,\n",
    "# cross_validation などは、既に定義済みとして進めます。\n",
    "\n",
    "# 実験設定\n",
    "M = 5\n",
    "K = 5\n",
    "N = 500\n",
    "B=100\n",
    "r_mean = 0.8\n",
    "r_std = 0.1\n",
    "r_min = 0.5\n",
    "r_max = 1.1\n",
    "delta = 0.6\n",
    "z_range = 0.4\n",
    "\n",
    "lb, ub, bounds, range_bounds = create_bounds(M, r_min, r_max)\n",
    "\n",
    "# 実験を何回行うか（例：2回）\n",
    "num_experiments = 100\n",
    "\n",
    "so_sales_list_5 = []\n",
    "po_sales_list_5 = []\n",
    "true_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "quan95_sales_list_5 = []\n",
    "true_quan95_sales_list_5 = []\n",
    "\n",
    "quan90_sales_list_5 = []\n",
    "true_quan90_sales_list_5 = []\n",
    "\n",
    "quan85_sales_list_5 = []\n",
    "true_quan85_sales_list_5 = []\n",
    "\n",
    "quan80_sales_list_5 = []\n",
    "true_quan80_sales_list_5 = []\n",
    "\n",
    "boot95_sales_list_5 = []\n",
    "true_boot95_sales_list_5 = []\n",
    "\n",
    "boot90_sales_list_5 = []\n",
    "true_boot90_sales_list_5 = []\n",
    "\n",
    "boot85_sales_list_5 = []\n",
    "true_boot85_sales_list_5 = []\n",
    "\n",
    "boot80_sales_list_5 = []\n",
    "true_boot80_sales_list_5 = []\n",
    "\n",
    "ebpa3_po_sales_list_5 = []\n",
    "true_ebpa3_po_sales_list_5 = []\n",
    "\n",
    "ebpa4_po_sales_list_5 = []\n",
    "true_ebpa4_po_sales_list_5 = []\n",
    "\n",
    "ebpa5_po_sales_list_5 = []\n",
    "true_ebpa5_po_sales_list_5 = []\n",
    "\n",
    "ebz_range_deff_list_5 = []\n",
    "ebpa3_range_deff_list_5 = []\n",
    "ebpa4_range_deff_list_5 = []\n",
    "ebpa5_range_deff_list_5 = []\n",
    "\n",
    "# ★ 追加： 全体学習モデルの R^2 を記録するリスト\n",
    "all_r2_list = []\n",
    "\n",
    "each_range_deff_3_list = []\n",
    "each_range_deff_4_list = []\n",
    "each_range_deff_5_list = []\n",
    "each_range_deff_6_list = []\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    np.random.seed(i + 6)\n",
    "    alpha, beta, X, Y = create_date(M, N, r_mean, r_std, delta)\n",
    "\n",
    "    tilda_coefs_list = []\n",
    "    tilda_intercepts_list = []\n",
    "    hat_coefs_list = []\n",
    "    hat_intercepts_list = []\n",
    "\n",
    "    # ----------- 1) KFold で tilda / hat を推定 (境界推定用) -----------\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # lr_tilda: trainデータで学習\n",
    "        lr_tilda = MultiOutputRegressor(LinearRegression())\n",
    "        lr_tilda.fit(X_train, y_train)\n",
    "\n",
    "        # tildaの係数・切片を保存\n",
    "        coefs = [est.coef_ for est in lr_tilda.estimators_]\n",
    "        intercepts = [est.intercept_ for est in lr_tilda.estimators_]\n",
    "        tilda_coefs_list.append(coefs)\n",
    "        tilda_intercepts_list.append(intercepts)\n",
    "\n",
    "        # lr_hat: testデータで学習\n",
    "        lr_hat = MultiOutputRegressor(LinearRegression())\n",
    "        lr_hat.fit(X_test, y_test)\n",
    "\n",
    "        hat_coefs = [est.coef_ for est in lr_hat.estimators_]\n",
    "        hat_intercepts = [est.intercept_ for est in lr_hat.estimators_]\n",
    "        hat_coefs_list.append(hat_coefs)\n",
    "        hat_intercepts_list.append(hat_intercepts)\n",
    "\n",
    "    # ----------- 3) 以下、既存の最適化ロジック -----------\n",
    "    # SO (真のパラメータを使った最適化)\n",
    "    so_sales, so_prices = sales_optimize(M, alpha, beta, bounds)\n",
    "\n",
    "    # PO (実データ X, Y で学習したモデルを使い最適化)\n",
    "    po_sales, po_prices = predict_optimize(M, X, Y, bounds)\n",
    "    true_po_sales = np.sum(sales_function(po_prices, alpha, beta))\n",
    "\n",
    "    so_sales_list_5.append(so_sales / so_sales)\n",
    "    po_sales_list_5.append(po_sales / so_sales)\n",
    "    true_po_sales_list_5.append(true_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(95%)\n",
    "    quan95_bounds = bound_quan(X, 0.95)\n",
    "    quan95_po_sales, quan95_po_prices = predict_optimize(M, X, Y, quan95_bounds)\n",
    "    true_quan95_po_sales = np.sum(sales_function(quan95_po_prices, alpha, beta))\n",
    "    quan95_sales_list_5.append(quan95_po_sales / so_sales)\n",
    "    true_quan95_sales_list_5.append(true_quan95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    quan90_bounds = bound_quan(X, 0.9)\n",
    "    quan90_po_sales, quan90_po_prices = predict_optimize(M, X, Y, quan90_bounds)\n",
    "    true_quan90_po_sales = np.sum(sales_function(quan90_po_prices, alpha, beta))\n",
    "    quan90_sales_list_5.append(quan90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_quan90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    quan85_bounds = bound_quan(X, 0.85)\n",
    "    quan85_po_sales, quan85_po_prices = predict_optimize(M, X, Y, quan85_bounds)\n",
    "    true_quan85_po_sales = np.sum(sales_function(quan85_po_prices, alpha, beta))\n",
    "    quan85_sales_list_5.append(quan85_po_sales / so_sales)\n",
    "    true_quan85_sales_list_5.append(true_quan85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    quan80_bounds = bound_quan(X, 0.8)\n",
    "    quan80_po_sales, quan80_po_prices = predict_optimize(M, X, Y, quan80_bounds)\n",
    "    true_quan80_po_sales = np.sum(sales_function(quan80_po_prices, alpha, beta))\n",
    "    quan80_sales_list_5.append(quan80_po_sales / so_sales)\n",
    "    true_quan80_sales_list_5.append(true_quan80_po_sales / so_sales)\n",
    "\n",
    "    # BOOt(95%)\n",
    "    boot95_bounds = bound_boot(X, 0.95)\n",
    "    boot95_po_sales, boot95_po_prices = predict_optimize(M, X, Y, boot95_bounds)\n",
    "    true_boot95_po_sales = np.sum(sales_function(boot95_po_prices, alpha, beta))\n",
    "    boot95_sales_list_5.append(boot95_po_sales / so_sales)\n",
    "    true_boot95_sales_list_5.append(true_boot95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    boot90_bounds = bound_boot(X, 0.9)\n",
    "    boot90_po_sales, boot90_po_prices = predict_optimize(M, X, Y, boot90_bounds)\n",
    "    true_boot90_po_sales = np.sum(sales_function(boot90_po_prices, alpha, beta))\n",
    "    boot90_sales_list_5.append(boot90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_boot90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    boot85_bounds = bound_boot(X, 0.85)\n",
    "    boot85_po_sales, boot85_po_prices = predict_optimize(M, X, Y, boot85_bounds)\n",
    "    true_boot85_po_sales = np.sum(sales_function(boot85_po_prices, alpha, beta))\n",
    "    boot85_sales_list_5.append(boot85_po_sales / so_sales)\n",
    "    true_boot85_sales_list_5.append(true_boot85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    boot80_bounds = bound_boot(X, 0.8)\n",
    "    boot80_po_sales, boot80_po_prices = predict_optimize(M, X, Y, boot80_bounds)\n",
    "    true_boot80_po_sales = np.sum(sales_function(boot80_po_prices, alpha, beta))\n",
    "    boot80_sales_list_5.append(boot80_po_sales / so_sales)\n",
    "    true_boot80_sales_list_5.append(true_boot80_po_sales / so_sales)\n",
    "\n",
    "    # EBZ\n",
    "    ebz_val, ebz_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.6,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebz_po_sales, ebz_po_prices = predict_optimize(M, X, Y, ebz_bounds)\n",
    "    true_ebz_po_sales = np.sum(sales_function(ebz_po_prices, alpha, beta))\n",
    "    ebz_po_sales_list_5.append(ebz_po_sales / so_sales)\n",
    "    true_ebz_po_sales_list_5.append(true_ebz_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.3)\n",
    "    ebpa3_val, ebpa3_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.30,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa3_po_sales, ebpa3_po_prices = predict_optimize(M, X, Y, ebpa3_bounds)\n",
    "    true_ebpa3_po_sales = np.sum(sales_function(ebpa3_po_prices, alpha, beta))\n",
    "    ebpa3_po_sales_list_5.append(ebpa3_po_sales / so_sales)\n",
    "    true_ebpa3_po_sales_list_5.append(true_ebpa3_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.4)\n",
    "    ebpa4_val, ebpa4_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.40,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa4_po_sales, ebpa4_po_prices = predict_optimize(M, X, Y, ebpa4_bounds)\n",
    "    true_ebpa4_po_sales = np.sum(sales_function(ebpa4_po_prices, alpha, beta))\n",
    "    ebpa4_po_sales_list_5.append(ebpa4_po_sales / so_sales)\n",
    "    true_ebpa4_po_sales_list_5.append(true_ebpa4_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.5)\n",
    "    ebpa5_val, ebpa5_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.50,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa5_po_sales, ebpa5_po_prices = predict_optimize(M, X, Y, ebpa5_bounds)\n",
    "    true_ebpa5_po_sales = np.sum(sales_function(ebpa5_po_prices, alpha, beta))\n",
    "    ebpa5_po_sales_list_5.append(ebpa5_po_sales / so_sales)\n",
    "    true_ebpa5_po_sales_list_5.append(true_ebpa5_po_sales / so_sales)\n",
    "\n",
    "    ebz_range_deff_list_ = [ebz_bounds[i][1] - ebz_bounds[i][0] for i in range(M)]\n",
    "    ebpa3_range_deff_list_ = [ebpa3_bounds[i][1] - ebpa3_bounds[i][0] for i in range(M)]\n",
    "    ebpa4_range_deff_list_ = [ebpa4_bounds[i][1] - ebpa4_bounds[i][0] for i in range(M)]\n",
    "    ebpa5_range_deff_list_ = [ebpa5_bounds[i][1] - ebpa5_bounds[i][0] for i in range(M)]\n",
    "\n",
    "    ebz_range_deff_list_5.append(np.sum(ebz_range_deff_list_))\n",
    "    ebpa3_range_deff_list_5.append(np.sum(ebpa3_range_deff_list_))\n",
    "    ebpa4_range_deff_list_5.append(np.sum(ebpa4_range_deff_list_))\n",
    "    ebpa5_range_deff_list_5.append(np.sum(ebpa5_range_deff_list_))\n",
    "\n",
    "    test_lr = MultiOutputRegressor(LinearRegression())\n",
    "    test_lr.fit(X, Y)\n",
    "    test_coefs = [estimate.coef_ for estimate in test_lr.estimators_]\n",
    "    test_intercepts = [estimate.intercept_ for estimate in test_lr.estimators_]\n",
    "\n",
    "    all_pred = test_lr.predict(X)\n",
    "    for k, estimator in enumerate(test_lr.estimators_):\n",
    "        # k番目の列（= k番目の出力）の真の値と予測値を取り出し\n",
    "        y_true_k = Y[:, k]\n",
    "        y_pred_k = all_pred[:, k]\n",
    "        # print(f\"y_true_k: {y_true_k}\")\n",
    "        # print(f\"y_pred_k: {y_pred_k}\")\n",
    "        # print(y_true_k == y_pred_k)\n",
    "        # print(estimator)\n",
    "        # RMSE の計算\n",
    "        # R^2 の計算\n",
    "        r2_k = r2_score(y_true_k, y_pred_k)\n",
    "\n",
    "        all_r2_list.append(r2_k)\n",
    "\n",
    "    # ★ 追加：各商品の範囲の差を記録\n",
    "    for j in range(M):\n",
    "        each_range_deff_6_list.append(ebz_bounds[j][1] - ebz_bounds[j][0])\n",
    "        each_range_deff_3_list.append(ebpa3_bounds[j][1] - ebpa3_bounds[j][0])\n",
    "        each_range_deff_4_list.append(ebpa4_bounds[j][1] - ebpa4_bounds[j][0])\n",
    "        each_range_deff_5_list.append(ebpa5_bounds[j][1] - ebpa5_bounds[j][0])\n",
    "\n",
    "# -------------------------------\n",
    "# 最後に全体データ学習モデルの R^2 を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 以下はお使いの既存の関数群を想定 -----\n",
    "# create_date, create_bounds, sales_optimize, predict_optimize,\n",
    "# sales_function, bound_quan, estimate_bounds_penalty_nelder_all,\n",
    "# cross_validation などは、既に定義済みとして進めます。\n",
    "\n",
    "# 実験設定\n",
    "M = 5\n",
    "K = 5\n",
    "N = 500\n",
    "B=100\n",
    "r_mean = 0.8\n",
    "r_std = 0.1\n",
    "r_min = 0.5\n",
    "r_max = 1.1\n",
    "delta = 0.6\n",
    "z_range = 0.4\n",
    "\n",
    "lb, ub, bounds, range_bounds = create_bounds(M, r_min, r_max)\n",
    "\n",
    "# 実験を何回行うか（例：2回）\n",
    "num_experiments = 100\n",
    "\n",
    "so_sales_list_5 = []\n",
    "po_sales_list_5 = []\n",
    "true_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "quan95_sales_list_5 = []\n",
    "true_quan95_sales_list_5 = []\n",
    "\n",
    "quan90_sales_list_5 = []\n",
    "true_quan90_sales_list_5 = []\n",
    "\n",
    "quan85_sales_list_5 = []\n",
    "true_quan85_sales_list_5 = []\n",
    "\n",
    "quan80_sales_list_5 = []\n",
    "true_quan80_sales_list_5 = []\n",
    "\n",
    "boot95_sales_list_5 = []\n",
    "true_boot95_sales_list_5 = []\n",
    "\n",
    "boot90_sales_list_5 = []\n",
    "true_boot90_sales_list_5 = []\n",
    "\n",
    "boot85_sales_list_5 = []\n",
    "true_boot85_sales_list_5 = []\n",
    "\n",
    "boot80_sales_list_5 = []\n",
    "true_boot80_sales_list_5 = []\n",
    "\n",
    "ebpa3_po_sales_list_5 = []\n",
    "true_ebpa3_po_sales_list_5 = []\n",
    "\n",
    "ebpa4_po_sales_list_5 = []\n",
    "true_ebpa4_po_sales_list_5 = []\n",
    "\n",
    "ebpa5_po_sales_list_5 = []\n",
    "true_ebpa5_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "ebz_range_deff_list_5 = []\n",
    "ebpa3_range_deff_list_5 = []\n",
    "ebpa4_range_deff_list_5 = []\n",
    "ebpa5_range_deff_list_5 = []\n",
    "\n",
    "# ★ 追加： 全体学習モデルの R^2 を記録するリスト\n",
    "all_r2_list = []\n",
    "\n",
    "each_range_deff_3_list = []\n",
    "each_range_deff_4_list = []\n",
    "each_range_deff_5_list = []\n",
    "each_range_deff_6_list = []\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    np.random.seed(i + 6)\n",
    "    alpha, beta, X, Y = create_date(M, N, r_mean, r_std, delta)\n",
    "\n",
    "    tilda_coefs_list = []\n",
    "    tilda_intercepts_list = []\n",
    "    hat_coefs_list = []\n",
    "    hat_intercepts_list = []\n",
    "\n",
    "    # ----------- 1) KFold で tilda / hat を推定 (境界推定用) -----------\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # lr_tilda: trainデータで学習\n",
    "        lr_tilda = MultiOutputRegressor(LinearRegression())\n",
    "        lr_tilda.fit(X_train, y_train)\n",
    "\n",
    "        # tildaの係数・切片を保存\n",
    "        coefs = [est.coef_ for est in lr_tilda.estimators_]\n",
    "        intercepts = [est.intercept_ for est in lr_tilda.estimators_]\n",
    "        tilda_coefs_list.append(coefs)\n",
    "        tilda_intercepts_list.append(intercepts)\n",
    "\n",
    "        # lr_hat: testデータで学習\n",
    "        lr_hat = MultiOutputRegressor(LinearRegression())\n",
    "        lr_hat.fit(X_test, y_test)\n",
    "\n",
    "        hat_coefs = [est.coef_ for est in lr_hat.estimators_]\n",
    "        hat_intercepts = [est.intercept_ for est in lr_hat.estimators_]\n",
    "        hat_coefs_list.append(hat_coefs)\n",
    "        hat_intercepts_list.append(hat_intercepts)\n",
    "\n",
    "    # ----------- 3) 以下、既存の最適化ロジック -----------\n",
    "    # SO (真のパラメータを使った最適化)\n",
    "    so_sales, so_prices = sales_optimize(M, alpha, beta, bounds)\n",
    "\n",
    "    # PO (実データ X, Y で学習したモデルを使い最適化)\n",
    "    po_sales, po_prices = predict_optimize(M, X, Y, bounds)\n",
    "    true_po_sales = np.sum(sales_function(po_prices, alpha, beta))\n",
    "\n",
    "    so_sales_list_5.append(so_sales / so_sales)\n",
    "    po_sales_list_5.append(po_sales / so_sales)\n",
    "    true_po_sales_list_5.append(true_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(95%)\n",
    "    quan95_bounds = bound_quan(X, 0.95)\n",
    "    quan95_po_sales, quan95_po_prices = predict_optimize(M, X, Y, quan95_bounds)\n",
    "    true_quan95_po_sales = np.sum(sales_function(quan95_po_prices, alpha, beta))\n",
    "    quan95_sales_list_5.append(quan95_po_sales / so_sales)\n",
    "    true_quan95_sales_list_5.append(true_quan95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    quan90_bounds = bound_quan(X, 0.9)\n",
    "    quan90_po_sales, quan90_po_prices = predict_optimize(M, X, Y, quan90_bounds)\n",
    "    true_quan90_po_sales = np.sum(sales_function(quan90_po_prices, alpha, beta))\n",
    "    quan90_sales_list_5.append(quan90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_quan90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    quan85_bounds = bound_quan(X, 0.85)\n",
    "    quan85_po_sales, quan85_po_prices = predict_optimize(M, X, Y, quan85_bounds)\n",
    "    true_quan85_po_sales = np.sum(sales_function(quan85_po_prices, alpha, beta))\n",
    "    quan85_sales_list_5.append(quan85_po_sales / so_sales)\n",
    "    true_quan85_sales_list_5.append(true_quan85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    quan80_bounds = bound_quan(X, 0.8)\n",
    "    quan80_po_sales, quan80_po_prices = predict_optimize(M, X, Y, quan80_bounds)\n",
    "    true_quan80_po_sales = np.sum(sales_function(quan80_po_prices, alpha, beta))\n",
    "    quan80_sales_list_5.append(quan80_po_sales / so_sales)\n",
    "    true_quan80_sales_list_5.append(true_quan80_po_sales / so_sales)\n",
    "\n",
    "    # BOOt(95%)\n",
    "    boot95_bounds = bound_boot(X, 0.95)\n",
    "    boot95_po_sales, boot95_po_prices = predict_optimize(M, X, Y, boot95_bounds)\n",
    "    true_boot95_po_sales = np.sum(sales_function(boot95_po_prices, alpha, beta))\n",
    "    boot95_sales_list_5.append(boot95_po_sales / so_sales)\n",
    "    true_boot95_sales_list_5.append(true_boot95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    boot90_bounds = bound_boot(X, 0.9)\n",
    "    boot90_po_sales, boot90_po_prices = predict_optimize(M, X, Y, boot90_bounds)\n",
    "    true_boot90_po_sales = np.sum(sales_function(boot90_po_prices, alpha, beta))\n",
    "    boot90_sales_list_5.append(boot90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_boot90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    boot85_bounds = bound_boot(X, 0.85)\n",
    "    boot85_po_sales, boot85_po_prices = predict_optimize(M, X, Y, boot85_bounds)\n",
    "    true_boot85_po_sales = np.sum(sales_function(boot85_po_prices, alpha, beta))\n",
    "    boot85_sales_list_5.append(boot85_po_sales / so_sales)\n",
    "    true_boot85_sales_list_5.append(true_boot85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    boot80_bounds = bound_boot(X, 0.8)\n",
    "    boot80_po_sales, boot80_po_prices = predict_optimize(M, X, Y, boot80_bounds)\n",
    "    true_boot80_po_sales = np.sum(sales_function(boot80_po_prices, alpha, beta))\n",
    "    boot80_sales_list_5.append(boot80_po_sales / so_sales)\n",
    "    true_boot80_sales_list_5.append(true_boot80_po_sales / so_sales)\n",
    "\n",
    "    # EBZ\n",
    "    ebz_val, ebz_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.6,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebz_po_sales, ebz_po_prices = predict_optimize(M, X, Y, ebz_bounds)\n",
    "    true_ebz_po_sales = np.sum(sales_function(ebz_po_prices, alpha, beta))\n",
    "    ebz_po_sales_list_5.append(ebz_po_sales / so_sales)\n",
    "    true_ebz_po_sales_list_5.append(true_ebz_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.3)\n",
    "    ebpa3_val, ebpa3_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.30,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa3_po_sales, ebpa3_po_prices = predict_optimize(M, X, Y, ebpa3_bounds)\n",
    "    true_ebpa3_po_sales = np.sum(sales_function(ebpa3_po_prices, alpha, beta))\n",
    "    ebpa3_po_sales_list_5.append(ebpa3_po_sales / so_sales)\n",
    "    true_ebpa3_po_sales_list_5.append(true_ebpa3_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.4)\n",
    "    ebpa4_val, ebpa4_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.40,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa4_po_sales, ebpa4_po_prices = predict_optimize(M, X, Y, ebpa4_bounds)\n",
    "    true_ebpa4_po_sales = np.sum(sales_function(ebpa4_po_prices, alpha, beta))\n",
    "    ebpa4_po_sales_list_5.append(ebpa4_po_sales / so_sales)\n",
    "    true_ebpa4_po_sales_list_5.append(true_ebpa4_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.5)\n",
    "    ebpa5_val, ebpa5_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.50,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa5_po_sales, ebpa5_po_prices = predict_optimize(M, X, Y, ebpa5_bounds)\n",
    "    true_ebpa5_po_sales = np.sum(sales_function(ebpa5_po_prices, alpha, beta))\n",
    "    ebpa5_po_sales_list_5.append(ebpa5_po_sales / so_sales)\n",
    "    true_ebpa5_po_sales_list_5.append(true_ebpa5_po_sales / so_sales)\n",
    "\n",
    "    ebz_range_deff_list_ = [ebz_bounds[i][1] - ebz_bounds[i][0] for i in range(M)]\n",
    "    ebpa3_range_deff_list_ = [ebpa3_bounds[i][1] - ebpa3_bounds[i][0] for i in range(M)]\n",
    "    ebpa4_range_deff_list_ = [ebpa4_bounds[i][1] - ebpa4_bounds[i][0] for i in range(M)]\n",
    "    ebpa5_range_deff_list_ = [ebpa5_bounds[i][1] - ebpa5_bounds[i][0] for i in range(M)]\n",
    "\n",
    "    ebz_range_deff_list_5.append(np.sum(ebz_range_deff_list_))\n",
    "    ebpa3_range_deff_list_5.append(np.sum(ebpa3_range_deff_list_))\n",
    "    ebpa4_range_deff_list_5.append(np.sum(ebpa4_range_deff_list_))\n",
    "    ebpa5_range_deff_list_5.append(np.sum(ebpa5_range_deff_list_))\n",
    "\n",
    "    test_lr = MultiOutputRegressor(LinearRegression())\n",
    "    test_lr.fit(X, Y)\n",
    "    test_coefs = [estimate.coef_ for estimate in test_lr.estimators_]\n",
    "    test_intercepts = [estimate.intercept_ for estimate in test_lr.estimators_]\n",
    "\n",
    "    all_pred = test_lr.predict(X)\n",
    "    for k, estimator in enumerate(test_lr.estimators_):\n",
    "        # k番目の列（= k番目の出力）の真の値と予測値を取り出し\n",
    "        y_true_k = Y[:, k]\n",
    "        y_pred_k = all_pred[:, k]\n",
    "        # print(f\"y_true_k: {y_true_k}\")\n",
    "        # print(f\"y_pred_k: {y_pred_k}\")\n",
    "        # print(y_true_k == y_pred_k)\n",
    "        # print(estimator)\n",
    "        # RMSE の計算\n",
    "        # R^2 の計算\n",
    "        r2_k = r2_score(y_true_k, y_pred_k)\n",
    "\n",
    "        all_r2_list.append(r2_k)\n",
    "\n",
    "    # ★ 追加：各商品の範囲の差を記録\n",
    "    for j in range(M):\n",
    "        each_range_deff_6_list.append(ebz_bounds[j][1] - ebz_bounds[j][0])\n",
    "        each_range_deff_3_list.append(ebpa3_bounds[j][1] - ebpa3_bounds[j][0])\n",
    "        each_range_deff_4_list.append(ebpa4_bounds[j][1] - ebpa4_bounds[j][0])\n",
    "        each_range_deff_5_list.append(ebpa5_bounds[j][1] - ebpa5_bounds[j][0])\n",
    "\n",
    "# -------------------------------\n",
    "# 最後に全体データ学習モデルの R^2 を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5_6にso_sales_list_5からebpa_range_diff_5までのリストを追加\n",
    "\n",
    "df_5_6 = pd.DataFrame(\n",
    "    {\n",
    "        \"so_sales_list_5\": so_sales_list_5,\n",
    "        \"po_sales_list_5\": po_sales_list_5,\n",
    "        \"true_po_sales_list_5\": true_po_sales_list_5,\n",
    "        \"ebz_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebz_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"quan95_sales_list_5\": quan95_sales_list_5,\n",
    "        \"true_quan95_sales_list_5\": true_quan95_sales_list_5,\n",
    "        \"quan90_sales_list_5\": quan90_sales_list_5,\n",
    "        \"true_quan90_sales_list_5\": true_quan90_sales_list_5,\n",
    "        \"quan85_sales_list_5\": quan85_sales_list_5,\n",
    "        \"true_quan85_sales_list_5\": true_quan85_sales_list_5,\n",
    "        \"quan80_sales_list_5\": quan80_sales_list_5,\n",
    "        \"true_quan80_sales_list_5\": true_quan80_sales_list_5,\n",
    "        \"boot95_sales_list_5\": boot95_sales_list_5,\n",
    "        \"true_boot95_sales_list_5\": true_boot95_sales_list_5,\n",
    "        \"boot90_sales_list_5\": boot90_sales_list_5,\n",
    "        \"true_boot90_sales_list_5\": true_boot90_sales_list_5,\n",
    "        \"boot85_sales_list_5\": boot85_sales_list_5,\n",
    "        \"true_boot85_sales_list_5\": true_boot85_sales_list_5,\n",
    "        \"boot80_sales_list_5\": boot80_sales_list_5,\n",
    "        \"true_boot80_sales_list_5\": true_boot80_sales_list_5,\n",
    "        \"ebpa3_po_sales_list_5\": ebpa3_po_sales_list_5,\n",
    "        \"true_ebpa3_po_sales_list_5\": true_ebpa3_po_sales_list_5,\n",
    "        \"ebpa4_po_sales_list_5\": ebpa4_po_sales_list_5,\n",
    "        \"true_ebpa4_po_sales_list_5\": true_ebpa4_po_sales_list_5,\n",
    "        \"ebpa5_po_sales_list_5\": ebpa5_po_sales_list_5,\n",
    "        \"true_ebpa5_po_sales_list_5\": true_ebpa5_po_sales_list_5,\n",
    "        \"epa6_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebpa6_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"ebpa3_range_deff_list_5\": ebpa3_range_deff_list_5,\n",
    "        \"ebpa4_range_deff_list_5\": ebpa4_range_deff_list_5,\n",
    "        \"ebpa5_range_deff_list_5\": ebpa5_range_deff_list_5,\n",
    "        \"ebpa6_range_deff_list_5\": ebz_range_deff_list_5,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_6.to_csv(\"df_5_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_6_each = pd.DataFrame(\n",
    "    {\n",
    "        \"R2\": all_r2_list,\n",
    "        \"each_range_deff_3_list\": each_range_deff_3_list,\n",
    "        \"each_range_deff_4_list\": each_range_deff_4_list,\n",
    "        \"each_range_deff_5_list\": each_range_deff_5_list,\n",
    "        \"each_range_deff_6_list\": each_range_deff_6_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_6_each.to_csv(\"df_5_6_each.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 以下はお使いの既存の関数群を想定 -----\n",
    "# create_date, create_bounds, sales_optimize, predict_optimize,\n",
    "# sales_function, bound_quan, estimate_bounds_penalty_nelder_all,\n",
    "# cross_validation などは、既に定義済みとして進めます。\n",
    "\n",
    "# 実験設定\n",
    "M = 5\n",
    "K = 5\n",
    "N = 500\n",
    "B=100\n",
    "r_mean = 0.8\n",
    "r_std = 0.1\n",
    "r_min = 0.5\n",
    "r_max = 1.1\n",
    "delta = 0.8\n",
    "z_range = 0.4\n",
    "\n",
    "lb, ub, bounds, range_bounds = create_bounds(M, r_min, r_max)\n",
    "\n",
    "# 実験を何回行うか（例：2回）\n",
    "num_experiments = 100\n",
    "\n",
    "so_sales_list_5 = []\n",
    "po_sales_list_5 = []\n",
    "true_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "quan95_sales_list_5 = []\n",
    "true_quan95_sales_list_5 = []\n",
    "\n",
    "quan90_sales_list_5 = []\n",
    "true_quan90_sales_list_5 = []\n",
    "\n",
    "quan85_sales_list_5 = []\n",
    "true_quan85_sales_list_5 = []\n",
    "\n",
    "quan80_sales_list_5 = []\n",
    "true_quan80_sales_list_5 = []\n",
    "\n",
    "boot95_sales_list_5 = []\n",
    "true_boot95_sales_list_5 = []\n",
    "\n",
    "boot90_sales_list_5 = []\n",
    "true_boot90_sales_list_5 = []\n",
    "\n",
    "boot85_sales_list_5 = []\n",
    "true_boot85_sales_list_5 = []\n",
    "\n",
    "boot80_sales_list_5 = []\n",
    "true_boot80_sales_list_5 = []\n",
    "\n",
    "ebpa3_po_sales_list_5 = []\n",
    "true_ebpa3_po_sales_list_5 = []\n",
    "\n",
    "ebpa4_po_sales_list_5 = []\n",
    "true_ebpa4_po_sales_list_5 = []\n",
    "\n",
    "ebpa5_po_sales_list_5 = []\n",
    "true_ebpa5_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "ebz_range_deff_list_5 = []\n",
    "ebpa3_range_deff_list_5 = []\n",
    "ebpa4_range_deff_list_5 = []\n",
    "ebpa5_range_deff_list_5 = []\n",
    "\n",
    "# ★ 追加： 全体学習モデルの R^2 を記録するリスト\n",
    "all_r2_list = []\n",
    "\n",
    "each_range_deff_3_list = []\n",
    "each_range_deff_4_list = []\n",
    "each_range_deff_5_list = []\n",
    "each_range_deff_6_list = []\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    np.random.seed(i + 6)\n",
    "    alpha, beta, X, Y = create_date(M, N, r_mean, r_std, delta)\n",
    "\n",
    "    tilda_coefs_list = []\n",
    "    tilda_intercepts_list = []\n",
    "    hat_coefs_list = []\n",
    "    hat_intercepts_list = []\n",
    "\n",
    "    # ----------- 1) KFold で tilda / hat を推定 (境界推定用) -----------\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # lr_tilda: trainデータで学習\n",
    "        lr_tilda = MultiOutputRegressor(LinearRegression())\n",
    "        lr_tilda.fit(X_train, y_train)\n",
    "\n",
    "        # tildaの係数・切片を保存\n",
    "        coefs = [est.coef_ for est in lr_tilda.estimators_]\n",
    "        intercepts = [est.intercept_ for est in lr_tilda.estimators_]\n",
    "        tilda_coefs_list.append(coefs)\n",
    "        tilda_intercepts_list.append(intercepts)\n",
    "\n",
    "        # lr_hat: testデータで学習\n",
    "        lr_hat = MultiOutputRegressor(LinearRegression())\n",
    "        lr_hat.fit(X_test, y_test)\n",
    "\n",
    "        hat_coefs = [est.coef_ for est in lr_hat.estimators_]\n",
    "        hat_intercepts = [est.intercept_ for est in lr_hat.estimators_]\n",
    "        hat_coefs_list.append(hat_coefs)\n",
    "        hat_intercepts_list.append(hat_intercepts)\n",
    "\n",
    "    # ----------- 3) 以下、既存の最適化ロジック -----------\n",
    "    # SO (真のパラメータを使った最適化)\n",
    "    so_sales, so_prices = sales_optimize(M, alpha, beta, bounds)\n",
    "\n",
    "    # PO (実データ X, Y で学習したモデルを使い最適化)\n",
    "    po_sales, po_prices = predict_optimize(M, X, Y, bounds)\n",
    "    true_po_sales = np.sum(sales_function(po_prices, alpha, beta))\n",
    "\n",
    "    so_sales_list_5.append(so_sales / so_sales)\n",
    "    po_sales_list_5.append(po_sales / so_sales)\n",
    "    true_po_sales_list_5.append(true_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(95%)\n",
    "    quan95_bounds = bound_quan(X, 0.95)\n",
    "    quan95_po_sales, quan95_po_prices = predict_optimize(M, X, Y, quan95_bounds)\n",
    "    true_quan95_po_sales = np.sum(sales_function(quan95_po_prices, alpha, beta))\n",
    "    quan95_sales_list_5.append(quan95_po_sales / so_sales)\n",
    "    true_quan95_sales_list_5.append(true_quan95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    quan90_bounds = bound_quan(X, 0.9)\n",
    "    quan90_po_sales, quan90_po_prices = predict_optimize(M, X, Y, quan90_bounds)\n",
    "    true_quan90_po_sales = np.sum(sales_function(quan90_po_prices, alpha, beta))\n",
    "    quan90_sales_list_5.append(quan90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_quan90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    quan85_bounds = bound_quan(X, 0.85)\n",
    "    quan85_po_sales, quan85_po_prices = predict_optimize(M, X, Y, quan85_bounds)\n",
    "    true_quan85_po_sales = np.sum(sales_function(quan85_po_prices, alpha, beta))\n",
    "    quan85_sales_list_5.append(quan85_po_sales / so_sales)\n",
    "    true_quan85_sales_list_5.append(true_quan85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    quan80_bounds = bound_quan(X, 0.8)\n",
    "    quan80_po_sales, quan80_po_prices = predict_optimize(M, X, Y, quan80_bounds)\n",
    "    true_quan80_po_sales = np.sum(sales_function(quan80_po_prices, alpha, beta))\n",
    "    quan80_sales_list_5.append(quan80_po_sales / so_sales)\n",
    "    true_quan80_sales_list_5.append(true_quan80_po_sales / so_sales)\n",
    "\n",
    "    # BOOt(95%)\n",
    "    boot95_bounds = bound_boot(X, 0.95)\n",
    "    boot95_po_sales, boot95_po_prices = predict_optimize(M, X, Y, boot95_bounds)\n",
    "    true_boot95_po_sales = np.sum(sales_function(boot95_po_prices, alpha, beta))\n",
    "    boot95_sales_list_5.append(boot95_po_sales / so_sales)\n",
    "    true_boot95_sales_list_5.append(true_boot95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    boot90_bounds = bound_boot(X, 0.9)\n",
    "    boot90_po_sales, boot90_po_prices = predict_optimize(M, X, Y, boot90_bounds)\n",
    "    true_boot90_po_sales = np.sum(sales_function(boot90_po_prices, alpha, beta))\n",
    "    boot90_sales_list_5.append(boot90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_boot90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    boot85_bounds = bound_boot(X, 0.85)\n",
    "    boot85_po_sales, boot85_po_prices = predict_optimize(M, X, Y, boot85_bounds)\n",
    "    true_boot85_po_sales = np.sum(sales_function(boot85_po_prices, alpha, beta))\n",
    "    boot85_sales_list_5.append(boot85_po_sales / so_sales)\n",
    "    true_boot85_sales_list_5.append(true_boot85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    boot80_bounds = bound_boot(X, 0.8)\n",
    "    boot80_po_sales, boot80_po_prices = predict_optimize(M, X, Y, boot80_bounds)\n",
    "    true_boot80_po_sales = np.sum(sales_function(boot80_po_prices, alpha, beta))\n",
    "    boot80_sales_list_5.append(boot80_po_sales / so_sales)\n",
    "    true_boot80_sales_list_5.append(true_boot80_po_sales / so_sales)\n",
    "\n",
    "    # EBZ\n",
    "    ebz_val, ebz_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.6,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebz_po_sales, ebz_po_prices = predict_optimize(M, X, Y, ebz_bounds)\n",
    "    true_ebz_po_sales = np.sum(sales_function(ebz_po_prices, alpha, beta))\n",
    "    ebz_po_sales_list_5.append(ebz_po_sales / so_sales)\n",
    "    true_ebz_po_sales_list_5.append(true_ebz_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.3)\n",
    "    ebpa3_val, ebpa3_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.30,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa3_po_sales, ebpa3_po_prices = predict_optimize(M, X, Y, ebpa3_bounds)\n",
    "    true_ebpa3_po_sales = np.sum(sales_function(ebpa3_po_prices, alpha, beta))\n",
    "    ebpa3_po_sales_list_5.append(ebpa3_po_sales / so_sales)\n",
    "    true_ebpa3_po_sales_list_5.append(true_ebpa3_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.4)\n",
    "    ebpa4_val, ebpa4_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.40,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa4_po_sales, ebpa4_po_prices = predict_optimize(M, X, Y, ebpa4_bounds)\n",
    "    true_ebpa4_po_sales = np.sum(sales_function(ebpa4_po_prices, alpha, beta))\n",
    "    ebpa4_po_sales_list_5.append(ebpa4_po_sales / so_sales)\n",
    "    true_ebpa4_po_sales_list_5.append(true_ebpa4_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.5)\n",
    "    ebpa5_val, ebpa5_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.50,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa5_po_sales, ebpa5_po_prices = predict_optimize(M, X, Y, ebpa5_bounds)\n",
    "    true_ebpa5_po_sales = np.sum(sales_function(ebpa5_po_prices, alpha, beta))\n",
    "    ebpa5_po_sales_list_5.append(ebpa5_po_sales / so_sales)\n",
    "    true_ebpa5_po_sales_list_5.append(true_ebpa5_po_sales / so_sales)\n",
    "\n",
    "    ebz_range_deff_list_ = [ebz_bounds[i][1] - ebz_bounds[i][0] for i in range(M)]\n",
    "    ebpa3_range_deff_list_ = [ebpa3_bounds[i][1] - ebpa3_bounds[i][0] for i in range(M)]\n",
    "    ebpa4_range_deff_list_ = [ebpa4_bounds[i][1] - ebpa4_bounds[i][0] for i in range(M)]\n",
    "    ebpa5_range_deff_list_ = [ebpa5_bounds[i][1] - ebpa5_bounds[i][0] for i in range(M)]\n",
    "\n",
    "    ebz_range_deff_list_5.append(np.sum(ebz_range_deff_list_))\n",
    "    ebpa3_range_deff_list_5.append(np.sum(ebpa3_range_deff_list_))\n",
    "    ebpa4_range_deff_list_5.append(np.sum(ebpa4_range_deff_list_))\n",
    "    ebpa5_range_deff_list_5.append(np.sum(ebpa5_range_deff_list_))\n",
    "\n",
    "    test_lr = MultiOutputRegressor(LinearRegression())\n",
    "    test_lr.fit(X, Y)\n",
    "    test_coefs = [estimate.coef_ for estimate in test_lr.estimators_]\n",
    "    test_intercepts = [estimate.intercept_ for estimate in test_lr.estimators_]\n",
    "\n",
    "    all_pred = test_lr.predict(X)\n",
    "    for k, estimator in enumerate(test_lr.estimators_):\n",
    "        # k番目の列（= k番目の出力）の真の値と予測値を取り出し\n",
    "        y_true_k = Y[:, k]\n",
    "        y_pred_k = all_pred[:, k]\n",
    "        # print(f\"y_true_k: {y_true_k}\")\n",
    "        # print(f\"y_pred_k: {y_pred_k}\")\n",
    "        # print(y_true_k == y_pred_k)\n",
    "        # print(estimator)\n",
    "        # RMSE の計算\n",
    "        # R^2 の計算\n",
    "        r2_k = r2_score(y_true_k, y_pred_k)\n",
    "\n",
    "        all_r2_list.append(r2_k)\n",
    "\n",
    "    # ★ 追加：各商品の範囲の差を記録\n",
    "    for j in range(M):\n",
    "        each_range_deff_6_list.append(ebz_bounds[j][1] - ebz_bounds[j][0])\n",
    "        each_range_deff_3_list.append(ebpa3_bounds[j][1] - ebpa3_bounds[j][0])\n",
    "        each_range_deff_4_list.append(ebpa4_bounds[j][1] - ebpa4_bounds[j][0])\n",
    "        each_range_deff_5_list.append(ebpa5_bounds[j][1] - ebpa5_bounds[j][0])\n",
    "\n",
    "# -------------------------------\n",
    "# 最後に全体データ学習モデルの R^2 を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5_6にso_sales_list_5からebpa_range_diff_5までのリストを追加\n",
    "\n",
    "df_5_8 = pd.DataFrame(\n",
    "    {\n",
    "        \"so_sales_list_5\": so_sales_list_5,\n",
    "        \"po_sales_list_5\": po_sales_list_5,\n",
    "        \"true_po_sales_list_5\": true_po_sales_list_5,\n",
    "        \"ebz_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebz_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"quan95_sales_list_5\": quan95_sales_list_5,\n",
    "        \"true_quan95_sales_list_5\": true_quan95_sales_list_5,\n",
    "        \"quan90_sales_list_5\": quan90_sales_list_5,\n",
    "        \"true_quan90_sales_list_5\": true_quan90_sales_list_5,\n",
    "        \"quan85_sales_list_5\": quan85_sales_list_5,\n",
    "        \"true_quan85_sales_list_5\": true_quan85_sales_list_5,\n",
    "        \"quan80_sales_list_5\": quan80_sales_list_5,\n",
    "        \"true_quan80_sales_list_5\": true_quan80_sales_list_5,\n",
    "        \"boot95_sales_list_5\": boot95_sales_list_5,\n",
    "        \"true_boot95_sales_list_5\": true_boot95_sales_list_5,\n",
    "        \"boot90_sales_list_5\": boot90_sales_list_5,\n",
    "        \"true_boot90_sales_list_5\": true_boot90_sales_list_5,\n",
    "        \"boot85_sales_list_5\": boot85_sales_list_5,\n",
    "        \"true_boot85_sales_list_5\": true_boot85_sales_list_5,\n",
    "        \"boot80_sales_list_5\": boot80_sales_list_5,\n",
    "        \"true_boot80_sales_list_5\": true_boot80_sales_list_5,\n",
    "        \"ebpa3_po_sales_list_5\": ebpa3_po_sales_list_5,\n",
    "        \"true_ebpa3_po_sales_list_5\": true_ebpa3_po_sales_list_5,\n",
    "        \"ebpa4_po_sales_list_5\": ebpa4_po_sales_list_5,\n",
    "        \"true_ebpa4_po_sales_list_5\": true_ebpa4_po_sales_list_5,\n",
    "        \"ebpa5_po_sales_list_5\": ebpa5_po_sales_list_5,\n",
    "        \"true_ebpa5_po_sales_list_5\": true_ebpa5_po_sales_list_5,\n",
    "        \"epa6_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebpa6_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"ebpa3_range_deff_list_5\": ebpa3_range_deff_list_5,\n",
    "        \"ebpa4_range_deff_list_5\": ebpa4_range_deff_list_5,\n",
    "        \"ebpa5_range_deff_list_5\": ebpa5_range_deff_list_5,\n",
    "        \"ebpa6_range_deff_list_5\": ebz_range_deff_list_5,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_8.to_csv(\"df_5_8.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_8_each = pd.DataFrame(\n",
    "    {\n",
    "        \"R2\": all_r2_list,\n",
    "        \"each_range_deff_3_list\": each_range_deff_3_list,\n",
    "        \"each_range_deff_4_list\": each_range_deff_4_list,\n",
    "        \"each_range_deff_5_list\": each_range_deff_5_list,\n",
    "        \"each_range_deff_6_list\": each_range_deff_6_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_8_each.to_csv(\"df_5_8_each.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 以下はお使いの既存の関数群を想定 -----\n",
    "# create_date, create_bounds, sales_optimize, predict_optimize,\n",
    "# sales_function, bound_quan, estimate_bounds_penalty_nelder_all,\n",
    "# cross_validation などは、既に定義済みとして進めます。\n",
    "\n",
    "# 実験設定\n",
    "M = 5\n",
    "K = 5\n",
    "N = 500\n",
    "B=100\n",
    "r_mean = 0.8\n",
    "r_std = 0.1\n",
    "r_min = 0.5\n",
    "r_max = 1.1\n",
    "delta = 1.0\n",
    "z_range = 0.4\n",
    "\n",
    "lb, ub, bounds, range_bounds = create_bounds(M, r_min, r_max)\n",
    "\n",
    "# 実験を何回行うか（例：2回）\n",
    "num_experiments = 100\n",
    "\n",
    "so_sales_list_5 = []\n",
    "po_sales_list_5 = []\n",
    "true_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "quan95_sales_list_5 = []\n",
    "true_quan95_sales_list_5 = []\n",
    "\n",
    "quan90_sales_list_5 = []\n",
    "true_quan90_sales_list_5 = []\n",
    "\n",
    "quan85_sales_list_5 = []\n",
    "true_quan85_sales_list_5 = []\n",
    "\n",
    "quan80_sales_list_5 = []\n",
    "true_quan80_sales_list_5 = []\n",
    "\n",
    "boot95_sales_list_5 = []\n",
    "true_boot95_sales_list_5 = []\n",
    "\n",
    "boot90_sales_list_5 = []\n",
    "true_boot90_sales_list_5 = []\n",
    "\n",
    "boot85_sales_list_5 = []\n",
    "true_boot85_sales_list_5 = []\n",
    "\n",
    "boot80_sales_list_5 = []\n",
    "true_boot80_sales_list_5 = []\n",
    "\n",
    "ebpa3_po_sales_list_5 = []\n",
    "true_ebpa3_po_sales_list_5 = []\n",
    "\n",
    "ebpa4_po_sales_list_5 = []\n",
    "true_ebpa4_po_sales_list_5 = []\n",
    "\n",
    "ebpa5_po_sales_list_5 = []\n",
    "true_ebpa5_po_sales_list_5 = []\n",
    "\n",
    "ebz_po_sales_list_5 = []\n",
    "true_ebz_po_sales_list_5 = []\n",
    "\n",
    "ebz_range_deff_list_5 = []\n",
    "ebpa3_range_deff_list_5 = []\n",
    "ebpa4_range_deff_list_5 = []\n",
    "ebpa5_range_deff_list_5 = []\n",
    "\n",
    "# ★ 追加： 全体学習モデルの R^2 を記録するリスト\n",
    "all_r2_list = []\n",
    "\n",
    "each_range_deff_3_list = []\n",
    "each_range_deff_4_list = []\n",
    "each_range_deff_5_list = []\n",
    "each_range_deff_6_list = []\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    np.random.seed(i + 6)\n",
    "    alpha, beta, X, Y = create_date(M, N, r_mean, r_std, delta)\n",
    "\n",
    "    tilda_coefs_list = []\n",
    "    tilda_intercepts_list = []\n",
    "    hat_coefs_list = []\n",
    "    hat_intercepts_list = []\n",
    "\n",
    "    # ----------- 1) KFold で tilda / hat を推定 (境界推定用) -----------\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # lr_tilda: trainデータで学習\n",
    "        lr_tilda = MultiOutputRegressor(LinearRegression())\n",
    "        lr_tilda.fit(X_train, y_train)\n",
    "\n",
    "        # tildaの係数・切片を保存\n",
    "        coefs = [est.coef_ for est in lr_tilda.estimators_]\n",
    "        intercepts = [est.intercept_ for est in lr_tilda.estimators_]\n",
    "        tilda_coefs_list.append(coefs)\n",
    "        tilda_intercepts_list.append(intercepts)\n",
    "\n",
    "        # lr_hat: testデータで学習\n",
    "        lr_hat = MultiOutputRegressor(LinearRegression())\n",
    "        lr_hat.fit(X_test, y_test)\n",
    "\n",
    "        hat_coefs = [est.coef_ for est in lr_hat.estimators_]\n",
    "        hat_intercepts = [est.intercept_ for est in lr_hat.estimators_]\n",
    "        hat_coefs_list.append(hat_coefs)\n",
    "        hat_intercepts_list.append(hat_intercepts)\n",
    "\n",
    "    # ----------- 3) 以下、既存の最適化ロジック -----------\n",
    "    # SO (真のパラメータを使った最適化)\n",
    "    so_sales, so_prices = sales_optimize(M, alpha, beta, bounds)\n",
    "\n",
    "    # PO (実データ X, Y で学習したモデルを使い最適化)\n",
    "    po_sales, po_prices = predict_optimize(M, X, Y, bounds)\n",
    "    true_po_sales = np.sum(sales_function(po_prices, alpha, beta))\n",
    "\n",
    "    so_sales_list_5.append(so_sales / so_sales)\n",
    "    po_sales_list_5.append(po_sales / so_sales)\n",
    "    true_po_sales_list_5.append(true_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(95%)\n",
    "    quan95_bounds = bound_quan(X, 0.95)\n",
    "    quan95_po_sales, quan95_po_prices = predict_optimize(M, X, Y, quan95_bounds)\n",
    "    true_quan95_po_sales = np.sum(sales_function(quan95_po_prices, alpha, beta))\n",
    "    quan95_sales_list_5.append(quan95_po_sales / so_sales)\n",
    "    true_quan95_sales_list_5.append(true_quan95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    quan90_bounds = bound_quan(X, 0.9)\n",
    "    quan90_po_sales, quan90_po_prices = predict_optimize(M, X, Y, quan90_bounds)\n",
    "    true_quan90_po_sales = np.sum(sales_function(quan90_po_prices, alpha, beta))\n",
    "    quan90_sales_list_5.append(quan90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_quan90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    quan85_bounds = bound_quan(X, 0.85)\n",
    "    quan85_po_sales, quan85_po_prices = predict_optimize(M, X, Y, quan85_bounds)\n",
    "    true_quan85_po_sales = np.sum(sales_function(quan85_po_prices, alpha, beta))\n",
    "    quan85_sales_list_5.append(quan85_po_sales / so_sales)\n",
    "    true_quan85_sales_list_5.append(true_quan85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    quan80_bounds = bound_quan(X, 0.8)\n",
    "    quan80_po_sales, quan80_po_prices = predict_optimize(M, X, Y, quan80_bounds)\n",
    "    true_quan80_po_sales = np.sum(sales_function(quan80_po_prices, alpha, beta))\n",
    "    quan80_sales_list_5.append(quan80_po_sales / so_sales)\n",
    "    true_quan80_sales_list_5.append(true_quan80_po_sales / so_sales)\n",
    "\n",
    "    # BOOt(95%)\n",
    "    boot95_bounds = bound_boot(X, 0.95)\n",
    "    boot95_po_sales, boot95_po_prices = predict_optimize(M, X, Y, boot95_bounds)\n",
    "    true_boot95_po_sales = np.sum(sales_function(boot95_po_prices, alpha, beta))\n",
    "    boot95_sales_list_5.append(boot95_po_sales / so_sales)\n",
    "    true_boot95_sales_list_5.append(true_boot95_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(90%)\n",
    "    boot90_bounds = bound_boot(X, 0.9)\n",
    "    boot90_po_sales, boot90_po_prices = predict_optimize(M, X, Y, boot90_bounds)\n",
    "    true_boot90_po_sales = np.sum(sales_function(boot90_po_prices, alpha, beta))\n",
    "    boot90_sales_list_5.append(boot90_po_sales / so_sales)\n",
    "    true_quan90_sales_list_5.append(true_boot90_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(85%)\n",
    "    boot85_bounds = bound_boot(X, 0.85)\n",
    "    boot85_po_sales, boot85_po_prices = predict_optimize(M, X, Y, boot85_bounds)\n",
    "    true_boot85_po_sales = np.sum(sales_function(boot85_po_prices, alpha, beta))\n",
    "    boot85_sales_list_5.append(boot85_po_sales / so_sales)\n",
    "    true_boot85_sales_list_5.append(true_boot85_po_sales / so_sales)\n",
    "\n",
    "    # QUAN(80%)\n",
    "    boot80_bounds = bound_boot(X, 0.8)\n",
    "    boot80_po_sales, boot80_po_prices = predict_optimize(M, X, Y, boot80_bounds)\n",
    "    true_boot80_po_sales = np.sum(sales_function(boot80_po_prices, alpha, beta))\n",
    "    boot80_sales_list_5.append(boot80_po_sales / so_sales)\n",
    "    true_boot80_sales_list_5.append(true_boot80_po_sales / so_sales)\n",
    "\n",
    "    # EBZ\n",
    "    ebz_val, ebz_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.6,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebz_po_sales, ebz_po_prices = predict_optimize(M, X, Y, ebz_bounds)\n",
    "    true_ebz_po_sales = np.sum(sales_function(ebz_po_prices, alpha, beta))\n",
    "    ebz_po_sales_list_5.append(ebz_po_sales / so_sales)\n",
    "    true_ebz_po_sales_list_5.append(true_ebz_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.3)\n",
    "    ebpa3_val, ebpa3_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.30,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa3_po_sales, ebpa3_po_prices = predict_optimize(M, X, Y, ebpa3_bounds)\n",
    "    true_ebpa3_po_sales = np.sum(sales_function(ebpa3_po_prices, alpha, beta))\n",
    "    ebpa3_po_sales_list_5.append(ebpa3_po_sales / so_sales)\n",
    "    true_ebpa3_po_sales_list_5.append(true_ebpa3_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.4)\n",
    "    ebpa4_val, ebpa4_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.40,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa4_po_sales, ebpa4_po_prices = predict_optimize(M, X, Y, ebpa4_bounds)\n",
    "    true_ebpa4_po_sales = np.sum(sales_function(ebpa4_po_prices, alpha, beta))\n",
    "    ebpa4_po_sales_list_5.append(ebpa4_po_sales / so_sales)\n",
    "    true_ebpa4_po_sales_list_5.append(true_ebpa4_po_sales / so_sales)\n",
    "\n",
    "    # EBPA(0.5)\n",
    "    ebpa5_val, ebpa5_bounds = estimate_bounds_penalty_nelder_all(\n",
    "        range_bounds,\n",
    "        tilda_coefs_list,\n",
    "        tilda_intercepts_list,\n",
    "        hat_coefs_list,\n",
    "        hat_intercepts_list,\n",
    "        M,\n",
    "        K,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        0.50,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    )\n",
    "    ebpa5_po_sales, ebpa5_po_prices = predict_optimize(M, X, Y, ebpa5_bounds)\n",
    "    true_ebpa5_po_sales = np.sum(sales_function(ebpa5_po_prices, alpha, beta))\n",
    "    ebpa5_po_sales_list_5.append(ebpa5_po_sales / so_sales)\n",
    "    true_ebpa5_po_sales_list_5.append(true_ebpa5_po_sales / so_sales)\n",
    "\n",
    "    ebz_range_deff_list_ = [ebz_bounds[i][1] - ebz_bounds[i][0] for i in range(M)]\n",
    "    ebpa3_range_deff_list_ = [ebpa3_bounds[i][1] - ebpa3_bounds[i][0] for i in range(M)]\n",
    "    ebpa4_range_deff_list_ = [ebpa4_bounds[i][1] - ebpa4_bounds[i][0] for i in range(M)]\n",
    "    ebpa5_range_deff_list_ = [ebpa5_bounds[i][1] - ebpa5_bounds[i][0] for i in range(M)]\n",
    "\n",
    "    ebz_range_deff_list_5.append(np.sum(ebz_range_deff_list_))\n",
    "    ebpa3_range_deff_list_5.append(np.sum(ebpa3_range_deff_list_))\n",
    "    ebpa4_range_deff_list_5.append(np.sum(ebpa4_range_deff_list_))\n",
    "    ebpa5_range_deff_list_5.append(np.sum(ebpa5_range_deff_list_))\n",
    "\n",
    "    test_lr = MultiOutputRegressor(LinearRegression())\n",
    "    test_lr.fit(X, Y)\n",
    "    test_coefs = [estimate.coef_ for estimate in test_lr.estimators_]\n",
    "    test_intercepts = [estimate.intercept_ for estimate in test_lr.estimators_]\n",
    "\n",
    "    all_pred = test_lr.predict(X)\n",
    "    for k, estimator in enumerate(test_lr.estimators_):\n",
    "        # k番目の列（= k番目の出力）の真の値と予測値を取り出し\n",
    "        y_true_k = Y[:, k]\n",
    "        y_pred_k = all_pred[:, k]\n",
    "        # print(f\"y_true_k: {y_true_k}\")\n",
    "        # print(f\"y_pred_k: {y_pred_k}\")\n",
    "        # print(y_true_k == y_pred_k)\n",
    "        # print(estimator)\n",
    "        # RMSE の計算\n",
    "        # R^2 の計算\n",
    "        r2_k = r2_score(y_true_k, y_pred_k)\n",
    "\n",
    "        all_r2_list.append(r2_k)\n",
    "\n",
    "    # ★ 追加：各商品の範囲の差を記録\n",
    "    for j in range(M):\n",
    "        each_range_deff_6_list.append(ebz_bounds[j][1] - ebz_bounds[j][0])\n",
    "        each_range_deff_3_list.append(ebpa3_bounds[j][1] - ebpa3_bounds[j][0])\n",
    "        each_range_deff_4_list.append(ebpa4_bounds[j][1] - ebpa4_bounds[j][0])\n",
    "        each_range_deff_5_list.append(ebpa5_bounds[j][1] - ebpa5_bounds[j][0])\n",
    "\n",
    "# -------------------------------\n",
    "# 最後に全体データ学習モデルの R^2 を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5_6にso_sales_list_5からebpa_range_diff_5までのリストを追加\n",
    "\n",
    "df_5_10 = pd.DataFrame(\n",
    "    {\n",
    "        \"so_sales_list_5\": so_sales_list_5,\n",
    "        \"po_sales_list_5\": po_sales_list_5,\n",
    "        \"true_po_sales_list_5\": true_po_sales_list_5,\n",
    "        \"ebz_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebz_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"quan95_sales_list_5\": quan95_sales_list_5,\n",
    "        \"true_quan95_sales_list_5\": true_quan95_sales_list_5,\n",
    "        \"quan90_sales_list_5\": quan90_sales_list_5,\n",
    "        \"true_quan90_sales_list_5\": true_quan90_sales_list_5,\n",
    "        \"quan85_sales_list_5\": quan85_sales_list_5,\n",
    "        \"true_quan85_sales_list_5\": true_quan85_sales_list_5,\n",
    "        \"quan80_sales_list_5\": quan80_sales_list_5,\n",
    "        \"true_quan80_sales_list_5\": true_quan80_sales_list_5,\n",
    "        \"boot95_sales_list_5\": boot95_sales_list_5,\n",
    "        \"true_boot95_sales_list_5\": true_boot95_sales_list_5,\n",
    "        \"boot90_sales_list_5\": boot90_sales_list_5,\n",
    "        \"true_boot90_sales_list_5\": true_boot90_sales_list_5,\n",
    "        \"boot85_sales_list_5\": boot85_sales_list_5,\n",
    "        \"true_boot85_sales_list_5\": true_boot85_sales_list_5,\n",
    "        \"boot80_sales_list_5\": boot80_sales_list_5,\n",
    "        \"true_boot80_sales_list_5\": true_boot80_sales_list_5,\n",
    "        \"ebpa3_po_sales_list_5\": ebpa3_po_sales_list_5,\n",
    "        \"true_ebpa3_po_sales_list_5\": true_ebpa3_po_sales_list_5,\n",
    "        \"ebpa4_po_sales_list_5\": ebpa4_po_sales_list_5,\n",
    "        \"true_ebpa4_po_sales_list_5\": true_ebpa4_po_sales_list_5,\n",
    "        \"ebpa5_po_sales_list_5\": ebpa5_po_sales_list_5,\n",
    "        \"true_ebpa5_po_sales_list_5\": true_ebpa5_po_sales_list_5,\n",
    "        \"epa6_po_sales_list_5\": ebz_po_sales_list_5,\n",
    "        \"true_ebpa6_po_sales_list_5\": true_ebz_po_sales_list_5,\n",
    "        \"ebpa3_range_deff_list_5\": ebpa3_range_deff_list_5,\n",
    "        \"ebpa4_range_deff_list_5\": ebpa4_range_deff_list_5,\n",
    "        \"ebpa5_range_deff_list_5\": ebpa5_range_deff_list_5,\n",
    "        \"ebpa6_range_deff_list_5\": ebz_range_deff_list_5,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_10.to_csv(\"df_5_10.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_10_each = pd.DataFrame(\n",
    "    {\n",
    "        \"R2\": all_r2_list,\n",
    "        \"each_range_deff_3_list\": each_range_deff_3_list,\n",
    "        \"each_range_deff_4_list\": each_range_deff_4_list,\n",
    "        \"each_range_deff_5_list\": each_range_deff_5_list,\n",
    "        \"each_range_deff_6_list\": each_range_deff_6_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "# csvファイルに出力\n",
    "df_5_10_each.to_csv(\"df_5_10_each.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
